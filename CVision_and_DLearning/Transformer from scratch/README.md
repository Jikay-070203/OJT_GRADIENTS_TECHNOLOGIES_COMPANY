# ğŸ¤– Transformer From Scratch - PyTorch

Notebook nÃ y xÃ¢y dá»±ng mÃ´ hÃ¬nh **Transformer hoÃ n chá»‰nh tá»« Ä‘áº§u** báº±ng PyTorch, khÃ´ng dÃ¹ng cÃ¡c hÃ m cÃ³ sáºµn nhÆ° `nn.Transformer`. PhÃ¹ há»£p cho ngÆ°á»i muá»‘n hiá»ƒu sÃ¢u cÆ¡ cháº¿ attention, positional encoding, encoder/decoder vÃ  training loop.

---

## ğŸ“Œ Ná»™i dung chÃ­nh

### 1. **Embeddings**

- Word embedding vÃ  positional encoding.
- Káº¿t há»£p thÃ nh input embeddings.

### 2. **Scaled Dot-Product Attention**

- TÃ­nh toÃ¡n attention tá»« Q, K, V.
- Softmax vá»›i mask (tá»± Ä‘á»™ng che Ä‘i tÆ°Æ¡ng lai trong decoder).

### 3. **Multi-Head Attention**

- TÃ¡ch QKV thÃ nh nhiá»u "Ä‘áº§u" Ä‘á»ƒ há»c Ä‘a chiá»u.
- Ná»‘i láº¡i vÃ  chiáº¿u vá» Ä‘áº§u ra.

### 4. **Feedforward & LayerNorm**

- MLP hai táº§ng + Dropout.
- Residual connections + LayerNorm.

### 5. **Encoder & Decoder Block**

- Stack nhiá»u layer encoder vÃ  decoder.
- DÃ¹ng self-attention vÃ  cross-attention.

### 6. **Training**

- Dataset toy: dá»‹ch Ä‘Æ¡n giáº£n (vÃ­ dá»¥: tá»« sá»‘ sang chá»¯).
- Huáº¥n luyá»‡n mÃ´ hÃ¬nh dá»‹ch seq2seq.
- Mask padding, target shifting, loss.

---

## ğŸ§  Má»¥c tiÃªu há»c Ä‘Æ°á»£c:

- Hiá»ƒu sÃ¢u **cÆ¡ cháº¿ attention**.
- Tá»± viáº¿t mÃ´ hÃ¬nh Transformer khÃ´ng phá»¥ thuá»™c thÆ° viá»‡n ngoÃ i.
- Ãp dá»¥ng vÃ o cÃ¡c bÃ i toÃ¡n NLP nhÆ° dá»‹ch, tÃ³m táº¯t, sinh vÄƒn báº£n.

---

## âš™ï¸ YÃªu cáº§u:

- Python 3.7+
- PyTorch â‰¥ 1.10
- NumPy, Matplotlib
