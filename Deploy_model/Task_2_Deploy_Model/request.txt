###  Next Step: Deploy Model   

####  Tutorials:   
-  Calling API:  `tutorials/Quick_Deploy/ONNX/README.md` at `main` · `triton-inference-server/tutorials`  
-  Hosting:  `server/docs/getting_started/quickstart.md` at `main` · `triton-inference-server/server`  
-  Performance Benchmarking:  `server/docs/perf_benchmark/perf-analyzer-README.rst` at `main` · `triton-inference-server/server`  

####  References:   
-  GitHub - Triton Inference Server:  The Triton Inference Server provides an optimized cloud and on-premise inference solution.  
-  GitHub - Triton Tutorials:  This repository contains tutorials and examples for Triton Inference Server.  

###  Requirements:   
✅  Download and host  the Triton Inference Server image.  
✅ Write configuration files  to host the model.  
✅ Use Tutorial Link 1  to call the API and run inference → Expected result: "Done".  
✅ Push code to GitHub (upload everything available).  
✅ (Extra)Measure performance with all configurable settings (refer to Tutorial Link 3).  
