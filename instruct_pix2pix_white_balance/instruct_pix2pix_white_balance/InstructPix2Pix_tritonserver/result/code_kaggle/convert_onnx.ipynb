{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RUN KAGGLE AND COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install diffusers accelerate safetensors transformers\n",
    "!pip install onnxruntime-gpu onnx\n",
    "!pip install onnxruntime onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CONVERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n",
    "import onnx\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the model\n",
    "model_id = \"timbrooks/instruct-pix2pix\"\n",
    "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Set models to evaluation mode\n",
    "pipe.unet.eval()\n",
    "pipe.vae.eval()\n",
    "\n",
    "# Define dimensions\n",
    "batch_size = 1\n",
    "latent_channels = 4\n",
    "image_latent_channels = 4\n",
    "height = 64  # 512/8 due to vae_scale_factor=8\n",
    "width = 64\n",
    "sequence_length = 77\n",
    "hidden_size = 768\n",
    "\n",
    "# --- Export UNet ---\n",
    "dummy_latents = torch.randn(batch_size, latent_channels, height, width).to(\"cuda\").half()\n",
    "dummy_image_latents = torch.randn(batch_size, image_latent_channels, height, width).to(\"cuda\").half()\n",
    "dummy_timestep = torch.ones(batch_size).to(\"cuda\").half()\n",
    "dummy_encoder_hidden_states = torch.randn(batch_size, sequence_length, hidden_size).to(\"cuda\").half()\n",
    "dummy_model_input = torch.cat([dummy_latents, dummy_image_latents], dim=1)\n",
    "\n",
    "onnx_path_unet = \"instruct_pix2pix_unet.onnx\"\n",
    "torch.onnx.export(\n",
    "    pipe.unet,\n",
    "    (dummy_model_input, dummy_timestep, dummy_encoder_hidden_states),\n",
    "    onnx_path_unet,\n",
    "    input_names=[\"latent_model_input\", \"timestep\", \"encoder_hidden_states\"],\n",
    "    output_names=[\"noise_pred\"],\n",
    "    dynamic_axes={\n",
    "        \"latent_model_input\": {0: \"batch_size\"},\n",
    "        \"timestep\": {0: \"batch_size\"},\n",
    "        \"encoder_hidden_states\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"noise_pred\": {0: \"batch_size\"}\n",
    "    },\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    "    export_params=True\n",
    ")\n",
    "\n",
    "# --- Export VAE Encoder with Wrapper Logic ---\n",
    "class VAEEncoderWrapper(nn.Module):\n",
    "    def __init__(self, vae):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Mimic vae.encode() but return raw latents instead of distribution\n",
    "        encoder_output = self.vae.encode(image)\n",
    "        latents = encoder_output.latent_dist.mode()  # Use mode instead of sampling for determinism\n",
    "        return latents\n",
    "\n",
    "vae_encoder_wrapper = VAEEncoderWrapper(pipe.vae)\n",
    "vae_encoder_wrapper.eval()\n",
    "\n",
    "dummy_image = torch.randn(1, 3, 512, 512).to(\"cuda\").half()\n",
    "onnx_path_vae_encoder = \"instruct_pix2pix_vae_encoder.onnx\"\n",
    "torch.onnx.export(\n",
    "    vae_encoder_wrapper,\n",
    "    dummy_image,\n",
    "    onnx_path_vae_encoder,\n",
    "    input_names=[\"image\"],\n",
    "    output_names=[\"latents\"],\n",
    "    dynamic_axes={\"image\": {0: \"batch_size\"}, \"latents\": {0: \"batch_size\"}},\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    "    export_params=True\n",
    ")\n",
    "\n",
    "# --- Export VAE Decoder with Wrapper Logic ---\n",
    "class VAEDecoderWrapper(nn.Module):\n",
    "    def __init__(self, vae):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "        self.scaling_factor = vae.config.scaling_factor\n",
    "\n",
    "    def forward(self, latents):\n",
    "        # Mimic vae.decode() including scaling\n",
    "        scaled_latents = latents / self.scaling_factor\n",
    "        decoded = self.vae.decode(scaled_latents)\n",
    "        return decoded[0]  # Return the image tensor directly\n",
    "\n",
    "vae_decoder_wrapper = VAEDecoderWrapper(pipe.vae)\n",
    "vae_decoder_wrapper.eval()\n",
    "\n",
    "dummy_latents = torch.randn(1, 4, 64, 64).to(\"cuda\").half()\n",
    "onnx_path_vae_decoder = \"instruct_pix2pix_vae_decoder.onnx\"\n",
    "torch.onnx.export(\n",
    "    vae_decoder_wrapper,\n",
    "    dummy_latents,\n",
    "    onnx_path_vae_decoder,\n",
    "    input_names=[\"latents\"],\n",
    "    output_names=[\"decoded_image\"],\n",
    "    dynamic_axes={\"latents\": {0: \"batch_size\"}, \"decoded_image\": {0: \"batch_size\"}},\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    "    export_params=True\n",
    ")\n",
    "\n",
    "# Verify exports\n",
    "for path in [onnx_path_unet, onnx_path_vae_encoder, onnx_path_vae_decoder]:\n",
    "    model = onnx.load(path)\n",
    "    onnx.checker.check_model(model)\n",
    "    print(f\"Verified {path}\")\n",
    "\n",
    "print(\"Export completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
