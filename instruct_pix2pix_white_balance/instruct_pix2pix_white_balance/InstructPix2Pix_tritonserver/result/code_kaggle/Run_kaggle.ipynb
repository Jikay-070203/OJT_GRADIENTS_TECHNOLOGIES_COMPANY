{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T17:10:40.890753Z",
     "iopub.status.busy": "2025-03-23T17:10:40.890301Z",
     "iopub.status.idle": "2025-03-23T17:10:45.599133Z",
     "shell.execute_reply": "2025-03-23T17:10:45.598148Z",
     "shell.execute_reply.started": "2025-03-23T17:10:40.890716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install diffusers accelerate safetensors transformers\n",
    "!pip install onnxruntime-gpu onnx\n",
    "!pip install onnxruntime onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert_ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T17:12:59.936525Z",
     "iopub.status.busy": "2025-03-23T17:12:59.936082Z",
     "iopub.status.idle": "2025-03-23T17:14:13.180939Z",
     "shell.execute_reply": "2025-03-23T17:14:13.180064Z",
     "shell.execute_reply.started": "2025-03-23T17:12:59.936493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n",
    "import onnx\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the model\n",
    "model_id = \"timbrooks/instruct-pix2pix\"\n",
    "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Set models to evaluation mode\n",
    "pipe.unet.eval()\n",
    "pipe.vae.eval()\n",
    "\n",
    "# Define dimensions\n",
    "batch_size = 1\n",
    "latent_channels = 4\n",
    "image_latent_channels = 4\n",
    "height = 64  # 512/8 due to vae_scale_factor=8\n",
    "width = 64\n",
    "sequence_length = 77\n",
    "hidden_size = 768\n",
    "\n",
    "# --- Export UNet ---\n",
    "dummy_latents = torch.randn(batch_size, latent_channels, height, width).to(\"cuda\").half()\n",
    "dummy_image_latents = torch.randn(batch_size, image_latent_channels, height, width).to(\"cuda\").half()\n",
    "dummy_timestep = torch.ones(batch_size).to(\"cuda\").half()\n",
    "dummy_encoder_hidden_states = torch.randn(batch_size, sequence_length, hidden_size).to(\"cuda\").half()\n",
    "dummy_model_input = torch.cat([dummy_latents, dummy_image_latents], dim=1)\n",
    "\n",
    "onnx_path_unet = \"instruct_pix2pix_unet.onnx\"\n",
    "torch.onnx.export(\n",
    "    pipe.unet,\n",
    "    (dummy_model_input, dummy_timestep, dummy_encoder_hidden_states),\n",
    "    onnx_path_unet,\n",
    "    input_names=[\"latent_model_input\", \"timestep\", \"encoder_hidden_states\"],\n",
    "    output_names=[\"noise_pred\"],\n",
    "    dynamic_axes={\n",
    "        \"latent_model_input\": {0: \"batch_size\"},\n",
    "        \"timestep\": {0: \"batch_size\"},\n",
    "        \"encoder_hidden_states\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"noise_pred\": {0: \"batch_size\"}\n",
    "    },\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    "    export_params=True\n",
    ")\n",
    "\n",
    "# --- Export VAE Encoder with Wrapper Logic ---\n",
    "class VAEEncoderWrapper(nn.Module):\n",
    "    def __init__(self, vae):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Mimic vae.encode() but return raw latents instead of distribution\n",
    "        encoder_output = self.vae.encode(image)\n",
    "        latents = encoder_output.latent_dist.mode()  # Use mode instead of sampling for determinism\n",
    "        return latents\n",
    "\n",
    "vae_encoder_wrapper = VAEEncoderWrapper(pipe.vae)\n",
    "vae_encoder_wrapper.eval()\n",
    "\n",
    "dummy_image = torch.randn(1, 3, 512, 512).to(\"cuda\").half()\n",
    "onnx_path_vae_encoder = \"instruct_pix2pix_vae_encoder.onnx\"\n",
    "torch.onnx.export(\n",
    "    vae_encoder_wrapper,\n",
    "    dummy_image,\n",
    "    onnx_path_vae_encoder,\n",
    "    input_names=[\"image\"],\n",
    "    output_names=[\"latents\"],\n",
    "    dynamic_axes={\"image\": {0: \"batch_size\"}, \"latents\": {0: \"batch_size\"}},\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    "    export_params=True\n",
    ")\n",
    "\n",
    "# --- Export VAE Decoder with Wrapper Logic ---\n",
    "class VAEDecoderWrapper(nn.Module):\n",
    "    def __init__(self, vae):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "        self.scaling_factor = vae.config.scaling_factor\n",
    "\n",
    "    def forward(self, latents):\n",
    "        # Mimic vae.decode() including scaling\n",
    "        scaled_latents = latents / self.scaling_factor\n",
    "        decoded = self.vae.decode(scaled_latents)\n",
    "        return decoded[0]  # Return the image tensor directly\n",
    "\n",
    "vae_decoder_wrapper = VAEDecoderWrapper(pipe.vae)\n",
    "vae_decoder_wrapper.eval()\n",
    "\n",
    "dummy_latents = torch.randn(1, 4, 64, 64).to(\"cuda\").half()\n",
    "onnx_path_vae_decoder = \"instruct_pix2pix_vae_decoder.onnx\"\n",
    "torch.onnx.export(\n",
    "    vae_decoder_wrapper,\n",
    "    dummy_latents,\n",
    "    onnx_path_vae_decoder,\n",
    "    input_names=[\"latents\"],\n",
    "    output_names=[\"decoded_image\"],\n",
    "    dynamic_axes={\"latents\": {0: \"batch_size\"}, \"decoded_image\": {0: \"batch_size\"}},\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    "    export_params=True\n",
    ")\n",
    "\n",
    "# Verify exports\n",
    "for path in [onnx_path_unet, onnx_path_vae_encoder, onnx_path_vae_decoder]:\n",
    "    model = onnx.load(path)\n",
    "    onnx.checker.check_model(model)\n",
    "    print(f\"Verified {path}\")\n",
    "\n",
    "print(\"Export completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T17:17:35.921522Z",
     "iopub.status.busy": "2025-03-23T17:17:35.920776Z",
     "iopub.status.idle": "2025-03-23T17:22:02.834038Z",
     "shell.execute_reply": "2025-03-23T17:22:02.833163Z",
     "shell.execute_reply.started": "2025-03-23T17:17:35.921494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n",
    "from diffusers.utils import load_image\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "# Load the original pipeline for non-ONNX components\n",
    "model_id = \"timbrooks/instruct-pix2pix\"\n",
    "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Load ONNX models\n",
    "unet_session = ort.InferenceSession(\"instruct_pix2pix_unet.onnx\", providers=['CUDAExecutionProvider'])\n",
    "vae_encoder_session = ort.InferenceSession(\"instruct_pix2pix_vae_encoder.onnx\", providers=['CUDAExecutionProvider'])\n",
    "vae_decoder_session = ort.InferenceSession(\"instruct_pix2pix_vae_decoder.onnx\", providers=['CUDAExecutionProvider'])\n",
    "\n",
    "# Setup CLIP components (kept in PyTorch)\n",
    "tokenizer = pipe.tokenizer\n",
    "text_encoder = pipe.text_encoder\n",
    "text_encoder.eval()\n",
    "\n",
    "# Setup scheduler and image processor\n",
    "scheduler = pipe.scheduler\n",
    "image_processor = pipe.image_processor\n",
    "vae_scale_factor = pipe.vae_scale_factor\n",
    "\n",
    "# Parameters\n",
    "prompt = \"white balance\"\n",
    "negative_prompt = \"\"\n",
    "num_inference_steps = 10\n",
    "guidance_scale = 7.5\n",
    "image_guidance_scale = 1.5\n",
    "batch_size = 1\n",
    "height = 512\n",
    "width = 512\n",
    "do_classifier_free_guidance = guidance_scale > 1.0 and image_guidance_scale >= 1.0\n",
    "\n",
    "# Load and preprocess input image\n",
    "image = load_image(\"/kaggle/input/test-data/IMG_1685.png\").resize((512, 512))\n",
    "image_tensor = image_processor.preprocess(image).to(\"cuda\").half()\n",
    "\n",
    "# Encode image to latents using ONNX VAE encoder\n",
    "image_np = image_tensor.cpu().numpy().astype(np.float16)\n",
    "image_latents = vae_encoder_session.run([\"latents\"], {\"image\": image_np})[0]\n",
    "image_latents = torch.from_numpy(image_latents).to(\"cuda\").half()\n",
    "\n",
    "# Encode prompt using CLIP\n",
    "def encode_prompt(prompt, negative_prompt, device, num_images_per_prompt, do_classifier_free_guidance):\n",
    "    text_inputs = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        prompt_embeds = text_encoder(text_inputs.input_ids.to(device))[0]\n",
    "    \n",
    "    if do_classifier_free_guidance:\n",
    "        neg_inputs = tokenizer(negative_prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            negative_prompt_embeds = text_encoder(neg_inputs.input_ids.to(device))[0]\n",
    "        prompt_embeds = torch.cat([prompt_embeds, negative_prompt_embeds, negative_prompt_embeds])\n",
    "    \n",
    "    prompt_embeds = prompt_embeds.repeat(num_images_per_prompt, 1, 1)\n",
    "    return prompt_embeds\n",
    "\n",
    "prompt_embeds = encode_prompt(prompt, negative_prompt, \"cuda\", 1, do_classifier_free_guidance)\n",
    "\n",
    "# Prepare initial noise latents\n",
    "latents_shape = (batch_size, 4, height // vae_scale_factor, width // vae_scale_factor)\n",
    "latents = torch.randn(latents_shape, device=\"cuda\", dtype=torch.float16) * scheduler.init_noise_sigma\n",
    "\n",
    "# Prepare image latents for CFG\n",
    "if do_classifier_free_guidance:\n",
    "    uncond_image_latents = torch.zeros_like(image_latents)\n",
    "    image_latents = torch.cat([image_latents, image_latents, uncond_image_latents], dim=0)\n",
    "\n",
    "# Diffusion loop\n",
    "scheduler.set_timesteps(num_inference_steps, device=\"cuda\")\n",
    "timesteps = scheduler.timesteps\n",
    "\n",
    "for i, t in enumerate(timesteps):\n",
    "    # Prepare inputs for UNet\n",
    "    if do_classifier_free_guidance:\n",
    "        latent_model_input = torch.cat([latents] * 3)\n",
    "    else:\n",
    "        latent_model_input = latents\n",
    "    \n",
    "    # Scale latents\n",
    "    scaled_latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "    \n",
    "    # Concatenate with image latents\n",
    "    scaled_latent_model_input = torch.cat([scaled_latent_model_input, image_latents], dim=1)\n",
    "    \n",
    "    # Convert to numpy with float16\n",
    "    latent_input_np = scaled_latent_model_input.cpu().numpy().astype(np.float16)\n",
    "    timestep_np = torch.full((latent_model_input.shape[0],), t.item(), device=\"cuda\", dtype=torch.float16).cpu().numpy().astype(np.float16)\n",
    "    encoder_hidden_states_np = prompt_embeds.cpu().numpy().astype(np.float16)\n",
    "    \n",
    "    # Run UNet inference with ONNX\n",
    "    noise_pred = unet_session.run(\n",
    "        [\"noise_pred\"],\n",
    "        {\n",
    "            \"latent_model_input\": latent_input_np,\n",
    "            \"timestep\": timestep_np,\n",
    "            \"encoder_hidden_states\": encoder_hidden_states_np\n",
    "        }\n",
    "    )[0]\n",
    "    noise_pred = torch.from_numpy(noise_pred).to(\"cuda\").half()\n",
    "    \n",
    "    # Perform guidance\n",
    "    if do_classifier_free_guidance:\n",
    "        noise_pred_text, noise_pred_image, noise_pred_uncond = noise_pred.chunk(3)\n",
    "        noise_pred = (\n",
    "            noise_pred_uncond\n",
    "            + guidance_scale * (noise_pred_text - noise_pred_image)\n",
    "            + image_guidance_scale * (noise_pred_image - noise_pred_uncond)\n",
    "        )\n",
    "    \n",
    "    # Step the scheduler\n",
    "    latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "\n",
    "# Decode latents to image using ONNX VAE decoder\n",
    "latents_np = latents.cpu().numpy().astype(np.float16)\n",
    "decoded_image = vae_decoder_session.run([\"decoded_image\"], {\"latents\": latents_np})[0]\n",
    "decoded_image = torch.from_numpy(decoded_image).to(\"cuda\").half()\n",
    "\n",
    "# Post-process the output\n",
    "output_image = image_processor.postprocess(decoded_image, output_type=\"pil\")[0]\n",
    "\n",
    "# Save the result\n",
    "output_image.save(\"/kaggle/working/output_image.png\")\n",
    "print(\"Inference completed, image saved as 'output_image.png'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6835555,
     "sourceId": 10983308,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6842202,
     "sourceId": 10992534,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6944433,
     "sourceId": 11134288,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
